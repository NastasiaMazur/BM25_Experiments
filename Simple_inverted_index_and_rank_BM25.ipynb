{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKRltwDjuER83f2U6ULPKf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NastasiaMazur/BM25_Experiments/blob/main/Simple_inverted_index_and_rank_BM25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-k6qbiX1kJJ",
        "outputId": "b09b0cca-cc46-440b-b1ec-7a616147883e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.10/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This project made for data-structure class\n",
        "import time\n",
        "import os\n",
        "import string\n",
        "start = time.time()\n",
        "#from nltk.corpus import gutenberg\n",
        "from string import punctuation\n",
        "from rank_bm25 import BM25Okapi\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "#from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnInhvrZ1oP1",
        "outputId": "3f2c0953-51cc-4b2f-ce42-748fc5bb3d73"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fv44v1gA3F-j",
        "outputId": "5b662f34-9f08-41c4-ef90-a2819749fdc4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKGTlZnC1Oek",
        "outputId": "aceeddfa-ca97-4771-dd49-df022f5b38d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(1, -0.5857246240799496), (0, -0.6269517981612489)]\n",
            "Runtime of the program is 3.090391159057617\n"
          ]
        }
      ],
      "source": [
        "stop_words = stopwords.words('english') # The list with the stopwords\n",
        "for i in punctuation:\n",
        "    stop_words.append(i)    # We append at the stopwords list, the punctuation (expecting better results)\n",
        "\n",
        "# Initialize the inverted index and documents list\n",
        "the_inverted_index = dict()  # Key = The Words, value = a List with tuples (DocID, Count)\n",
        "docs = []  # The list to store the content of each book\n",
        "\n",
        "# Set the path to your Google Drive folder containing the books\n",
        "mypath = '/content/drive/My Drive/Documents'\n",
        "books_collection = os.listdir(mypath)\n",
        "\n",
        "# Read the content of each book and add it to the docs list\n",
        "for i in range(len(books_collection)):\n",
        "    with open(os.path.join(mypath, books_collection[i]), 'r', encoding='utf-8') as file:\n",
        "        docs.append(file.read())\n",
        "\n",
        "# Tokenize the content of each book\n",
        "docs = [wordpunct_tokenize(doc) for doc in docs]\n",
        "\n",
        "# Initialize the list to store the results after removing stopwords\n",
        "docs_with_stopping = []\n",
        "\n",
        "for i in range(len(docs)):\n",
        "    docs_with_stopping.append([])\n",
        "    for k in range(len(docs[i])):\n",
        "        if docs[i][k].lower() not in stop_words:  # Check if it is a stopword\n",
        "            docs_with_stopping[i].append(docs[i][k].lower())  # Store the word in lowercase\n",
        "\n",
        "# Build the inverted index\n",
        "for i in range(len(docs_with_stopping)):  # For every book\n",
        "    count_term = dict()  # Create a temporary dictionary\n",
        "    for term in docs_with_stopping[i]:\n",
        "        if term not in count_term:  # If word not in temporary dictionary\n",
        "            count_term[term] = 1\n",
        "        else:\n",
        "            count_term[term] += 1\n",
        "    for term in count_term:  # For every word in the temporary dictionary\n",
        "        if term not in the_inverted_index:  # If not in final index\n",
        "            the_inverted_index[term] = []\n",
        "        the_inverted_index[term].append((i, count_term[term]))  # Insert the results\n",
        "\n",
        "# User search phrase\n",
        "user_phrase = \"Best places for programmers all around the world\"  # Searching Phrase\n",
        "user_phrase_token = wordpunct_tokenize(user_phrase)  # Tokenize\n",
        "user_phrase_token = [word for word in user_phrase_token if not word in stop_words]  # Check for stopwords\n",
        "user_phrase_token = [word.lower() for word in user_phrase_token]  # Convert to lowercase\n",
        "\n",
        "# Using BM25 library to rank results\n",
        "bm25 = BM25Okapi(docs_with_stopping)\n",
        "doc_scores = bm25.get_scores(user_phrase_token)  # List with rank score for every book\n",
        "\n",
        "# Writing the results to a .txt file\n",
        "try:\n",
        "    with open(\"results.txt\", \"w\", encoding='utf-8') as f:\n",
        "        for key, value in the_inverted_index.items():\n",
        "            try:\n",
        "                f.write(str(key) + str(value) + \"\\n\")\n",
        "            except UnicodeEncodeError:\n",
        "                print(\"Can't encode the word: \" + str(key) + str(value))\n",
        "except Exception as e:\n",
        "    print(f\"Error at opening the file: {e}\")\n",
        "\n",
        "# Sort and print the final scores\n",
        "final_scores = []\n",
        "for i in range(len(doc_scores)):\n",
        "    final_scores.append((i, doc_scores[i]))\n",
        "\n",
        "final_scores.sort(reverse=True, key=lambda x: x[1])\n",
        "print(final_scores)  # Result is a sorted list with tuples (DocId, RankBm25 score)\n",
        "\n",
        "# Print runtime of the program\n",
        "end = time.time()\n",
        "print(f\"Runtime of the program is {end - start}\")"
      ]
    }
  ]
}